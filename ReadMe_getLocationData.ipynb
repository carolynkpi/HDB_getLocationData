{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReadMe: Get Location Data\n",
    "The code here is part of Phase 2 of the Singapore HDB Resale Prices project: <https://carolynkpi.shinyapps.io/app_hdb/>. \n",
    "\n",
    "The code gets location data for each HDB block. The data includes:\n",
    "\n",
    "1. Nearest MRT/LRT station\n",
    "2. Radial distance to nearest MRT station. \n",
    "3. Walking distance to nearest MRT station. \n",
    "\n",
    "4. Number of MRT/LRT stations nearby.\n",
    "5. Number of bus stations nearby.\n",
    "6. Number of schools nearby.\n",
    "7. Number of food places nearby. \n",
    "8. Number of shopping places nearby. \n",
    "\n",
    "I am using 3 Google APIs. Each API requires a separate key.  \n",
    "\n",
    "1. Google Maps Geocoding (Location data 1)\n",
    "2. Google Maps Distance Matrix (Location data 3)\n",
    "3. Google Places (Location data 4 -8)\n",
    "\n",
    "While I really **love** Google because it is **free**, there is a daily usage limit on each API.  \n",
    "\n",
    "1. Google Maps Geocoding - 2500 requests per day, 50 requests per second\n",
    "2. Google Maps Distance Matrix - 2500 elements per day, 100 elements per second. \n",
    "3. Google Places - 1000 request per day by default, upgradable to 150,000 elements per day for free but with billing enablement. \n",
    "\n",
    "With this constraint, the codes are designed to: \n",
    "- be run for several times across several days (to keep within usage limits)\n",
    "- save each data point as it is retrieved (so that returned data points are not lost if the code stops unexpectedly)\n",
    "- log failed requests separately for retrying later (to make sure retries do not burst the usage limits)  \n",
    "\n",
    "While the project is done mostly in R, this data acquisition via API is done using Python. \n",
    "There is no special reason for that, just for my convenience because of my familiarity with Python when it comes to API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json as json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data\n",
    "Let us first review our HDB data [Source: <https://data.gov.sg/dataset/resale-flat-prices>]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     month        town flat_type block        street_name storey_range  \\\n",
      "0  2015-01  ANG MO KIO    3 ROOM   174   ANG MO KIO AVE 4     07 TO 09   \n",
      "1  2015-01  ANG MO KIO    3 ROOM   541  ANG MO KIO AVE 10     01 TO 03   \n",
      "2  2015-01  ANG MO KIO    3 ROOM   163   ANG MO KIO AVE 4     01 TO 03   \n",
      "3  2015-01  ANG MO KIO    3 ROOM   446  ANG MO KIO AVE 10     01 TO 03   \n",
      "4  2015-01  ANG MO KIO    3 ROOM   557  ANG MO KIO AVE 10     07 TO 09   \n",
      "\n",
      "   floor_area_sqm      flat_model  lease_commence_date  remaining_lease  \\\n",
      "0            60.0        Improved                 1986               70   \n",
      "1            68.0  New Generation                 1981               65   \n",
      "2            69.0  New Generation                 1980               64   \n",
      "3            68.0  New Generation                 1979               63   \n",
      "4            68.0  New Generation                 1980               64   \n",
      "\n",
      "   resale_price  \n",
      "0      255000.0  \n",
      "1      275000.0  \n",
      "2      285000.0  \n",
      "3      290000.0  \n",
      "4      290000.0   \n",
      "\n",
      "Number of data points:  55979\n",
      "Number of unique data points: 8260\n"
     ]
    }
   ],
   "source": [
    "dataFile = 'resale_prices_2015_.csv'\n",
    "raw = pd.read_csv(dataFile)\n",
    "print(raw.head(), '\\n')\n",
    "address = 'Blk ' + raw['block'] + ' ' + raw['street_name'] \n",
    "print('Number of data points: ', len(address))\n",
    "print('Number of unique data points:', len(address.unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be eliminate redundancies in the API requests, each address (i.e. each HDB block) should only be queried once for each type of requests. \n",
    "\n",
    "Now let us look at the list of Singapore MRT/LRT stations [Source:<https://en.wikipedia.org/wiki/List_of_Singapore_MRT_stations> ]. The data is obtained from but Wikipedia, but I have done some manual editing to the data using Excel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Station Code Station Name  Planning Area      Region     NS     EW     CG  \\\n",
      "0         NS10    Admiralty      Woodlands       NORTH   True  False  False   \n",
      "1          EW9     Aljunied        Geylang     CENTRAL  False   True  False   \n",
      "2         NS16   Ang Mo Kio     Ang Mo Kio  NORTH-EAST   True  False  False   \n",
      "3          SE3        Bakau       Sengkang  NORTH-EAST  False  False  False   \n",
      "4          BP9      Bangkit  Bukit Panjang        WEST  False  False  False   \n",
      "\n",
      "      NE     CC     DT                 ...                  LRT/MRT  \\\n",
      "0  False  False  False                 ...                      MRT   \n",
      "1  False  False  False                 ...                      MRT   \n",
      "2  False  False  False                 ...                      MRT   \n",
      "3  False  False  False                 ...                      LRT   \n",
      "4  False  False  False                 ...                      LRT   \n",
      "\n",
      "   No of lines  No of lines opened  Opening 1 Opening 2  Opening 3  \\\n",
      "0            1                   1  10/2/1996       NaN        NaN   \n",
      "1            1                   1  4/11/1989       NaN        NaN   \n",
      "2            1                   1  7/11/1987       NaN        NaN   \n",
      "3            1                   1  18/1/2003       NaN        NaN   \n",
      "4            1                   1  6/11/1999       NaN        NaN   \n",
      "\n",
      "   Bus Interchange Bus Terminal Others Connection to other transport means  \n",
      "0            False        False  FALSE                                 NaN  \n",
      "1            False        False  FALSE                                 NaN  \n",
      "2             True        False  FALSE         Â Ang Mo Kio Bus Interchange  \n",
      "3            False        False  FALSE                                 NaN  \n",
      "4            False        False  FALSE                                 NaN  \n",
      "\n",
      "[5 rows x 24 columns] \n",
      "\n",
      "Number of MRT/LRT stations:  190\n"
     ]
    }
   ],
   "source": [
    "mrtFile = 'MRT.csv'\n",
    "mrt = pd.read_csv(mrtFile)\n",
    "print(mrt.head(), '\\n')\n",
    "print('Number of MRT/LRT stations: ', len(mrt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Strategy\n",
    "Finding the nearest MRT/LRT station and the distance is a little tricky. We can use the Google Maps Distance Matrix to get the walking distance from every HDB block to every MRT station, but that will result in 8260 HDB x 190 stations = 1,569,400 request, requiring 628 days = 1.7 years given the free usage limit! \n",
    "\n",
    "To work around this problem, I have taken the following Steps 1-4: \n",
    "\n",
    "1. **Get the coordinates for each HDB block and each MRT stations using Google Maps Geocoding API.** \n",
    "There will be 8260 HDB + 190 MRT = 8450  requests, requiring 3.4 days. \n",
    "2. **Calculate the radial distance from each HDB block to each MRT station. **\n",
    "There will be 8260 HDB x 190 MRT = 1,569,400 distances to calculate locally. I am using the Vincenty distance from the geopy library. This task can be run quickly.\n",
    "3. **Find the nearest MRT/LRT station to each HDB block by finding the minimal radial distance. ** \n",
    "4. **Get the walking distance from each HDB block to the nearest MRT/LRT station found in Step 3, using Google Maps Distance Matrix API. **\n",
    "There will be 8260 requests, requiring 3.3 days\n",
    "\n",
    "Steps 1-4 will get us Location Data 1 - 3. \n",
    "\n",
    "Location Data 4-8 will require direct query to the Google Places API (Step 5). \n",
    "Google classifies each place according to the types here: <https://developers.google.com/places/web-service/supported_types>. \n",
    "A close look at the different types tells us that we need to at least make 8 different types of queries to collect Location Data 4-8. Results are returned in batches of 20 up to 3 pages.  Hence, there will possibly be 8260 HDB x 8 types x 3 pages = 198,240 requests, requiring 1.3 days if we enable billing for Google API (*update:* we will see that this is not true in Step 5's code because the requests becomes limited by the inter-query sleep time instead of by the Google API). \n",
    "\n",
    "To complete location data acquisition, I took approximately 2 weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "The codes are written in a modular approach, i.e. the codes can be run separately and the results can be saved progressively.  This is to minimize repeat runs as the process is computationally expensive. \n",
    "\n",
    "**Files to be imported:** \n",
    "\n",
    "myFunctions.py: <link for myFunctions>. This file contains all the common functions necessary to query Google's API including tryGET, tryGETp, getParamString and getKeys. \n",
    "\n",
    "placesTracker.py: <link for placesTracker>. This file enforces a tracker that will track the usage of the Google Places API. This is because there is billing enabled to the Places API and I just wanted to make sure that I don't accidentally exceed the free 150,000 requests limit. But it turns out that wasn't necessary since the requests ended up being limited by the inter-query sleep time. This file is imported to myFunctions.py. It relies on a log file residing in the same project directory called 'PlacesTracker.txt'.  I have included a 'PlacesTracker - Master.txt' file to start off a new project. \n",
    "\n",
    "**Files for each step:**\n",
    "\n",
    "Step 1: <link for MRT> <link for HDB>\n",
    "\n",
    "Step 2: <link>\n",
    "\n",
    "Step 3: <link> \n",
    "\n",
    "Step 4: <link>\n",
    "\n",
    "Step 5: <link>\n",
    "\n",
    "Because some steps can be split to several separate runs, this file is used to combine their outputs. <br>\n",
    "Combine_Files: <link>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Results from Each Step\n",
    "Finally, the results from each step are integrated to produced one final output csv file for subsequent analysis of the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
